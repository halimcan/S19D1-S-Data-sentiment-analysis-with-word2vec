{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec ile Duygu Analizi\n",
    "\n",
    "### Egzersiz hedefleri:\n",
    "- Kelimeleri Word2Vec ile vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rme\n",
    "- Word2vec tarafÄ±ndan verilen kelime temsilini RNN'e besleme\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â–¶ï¸ Bu hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n ve kullandÄ±ÄŸÄ±nÄ±z ğŸ“š [Gensim - Word2Vec](https://radimrehurek.com/gensim/auto_examples/index.html) sÃ¼rÃ¼mÃ¼nÃ¼n â‰¥ 4.0 olduÄŸundan emin olun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:05:07.829379Z",
     "iopub.status.busy": "2026-02-15T10:05:07.827215Z",
     "iopub.status.idle": "2026-02-15T10:05:09.609983Z",
     "shell.execute_reply": "2026-02-15T10:05:09.605507Z",
     "shell.execute_reply.started": "2026-02-15T10:05:07.829284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==4.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veri\n",
    "\n",
    "\n",
    "â“ **Soru** â“ Ã–nce veriyi yÃ¼kleyelim. Fonksiyonun iÃ§inde neler olduÄŸunu anlamanÄ±za gerek yok, burada Ã¶nemli deÄŸil.\n",
    "\n",
    "âš ï¸ **UyarÄ±** âš ï¸ `load_data` fonksiyonunun `percentage_of_sentences` argÃ¼manÄ± var. BilgisayarÄ±nÄ±za baÄŸlÄ± olarak, Ã§ok fazla cÃ¼mle bilgisayarÄ±nÄ±zÄ±n yavaÅŸlamasÄ±na hatta donmasÄ±na neden olabilir - RAM'iniz taÅŸabilir. Bu nedenle, **cÃ¼mlelerin %10'u ile baÅŸlamalÄ±** ve bilgisayarÄ±nÄ±zÄ±n bunu kaldÄ±rÄ±p kaldÄ±rmadÄ±ÄŸÄ±nÄ± gÃ¶rmelisiniz. Aksi takdirde daha dÃ¼ÅŸÃ¼k bir sayÄ± ile tekrar Ã§alÄ±ÅŸtÄ±rÄ±n.\n",
    "\n",
    "âš ï¸ **UYARI** âš ï¸ **_En bÃ¼yÃ¼k RAM'e sahip kim_ oyununu oynamaya gerek yok!** AmaÃ§, prototip yapmak iÃ§in modellerinizi hÄ±zlÄ±ca Ã§alÄ±ÅŸtÄ±rmaktÄ±r. GerÃ§ek hayatta bile, hÄ±zlÄ±ca dÃ¶ngÃ¼ yapÄ±p hata ayÄ±klamak iÃ§in verilerinizin bir alt kÃ¼mesi ile baÅŸlamanÄ±z Ã¶nerilir. Bu nedenle sayÄ±yÄ± sadece en iyi doÄŸruluÄŸu elde etmek istiyorsanÄ±z artÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:05:09.626389Z",
     "iopub.status.busy": "2026-02-15T10:05:09.620457Z",
     "iopub.status.idle": "2026-02-15T10:05:10.073072Z",
     "shell.execute_reply": "2026-02-15T10:05:10.070593Z",
     "shell.execute_reply.started": "2026-02-15T10:05:09.626277Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:05:10.078991Z",
     "iopub.status.busy": "2026-02-15T10:05:10.076156Z",
     "iopub.status.idle": "2026-02-15T10:05:39.538541Z",
     "shell.execute_reply": "2026-02-15T10:05:39.536184Z",
     "shell.execute_reply.started": "2026-02-15T10:05:10.078866Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-15 13:05:12.532079: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-15 13:05:12.771145: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-15 13:05:13.076754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-15 13:05:13.379188: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-15 13:05:13.381546: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-15 13:05:13.834596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-15 13:05:18.029078: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Just run this cell to load the data ###\n",
    "###########################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "\n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "\n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "\n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "\n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ã–nceki egzersizde, bir Word2vec temsili eÄŸittiniz ve tÃ¼m eÄŸitim cÃ¼mlelerinizi RNN'e beslemek iÃ§in dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼nÃ¼z. Bu ÅŸeklin ilk adÄ±mÄ±nda gÃ¶sterildiÄŸi gibi:\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/06-DL/NLP/word2vec_representation.png\" alt=\"Word2Vec with RNN\" width=\"400px\" />\n",
    "\n",
    "\n",
    "\n",
    "â“ **Soru** â“ Burada, Ã¶nceki egzersizde yaptÄ±ÄŸÄ±nÄ±zÄ±n aynÄ±sÄ±nÄ± tekrar yapalÄ±m. Ä°lk olarak, eÄŸitim cÃ¼mleleriniz Ã¼zerinde bir word2vec modeli eÄŸitin (istediÄŸiniz argÃ¼manlarla). Bunu `word2vec` deÄŸiÅŸkeninde saklayÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:05:39.545449Z",
     "iopub.status.busy": "2026-02-15T10:05:39.543984Z",
     "iopub.status.idle": "2026-02-15T10:05:47.780380Z",
     "shell.execute_reply": "2026-02-15T10:05:47.778356Z",
     "shell.execute_reply.started": "2026-02-15T10:05:39.545355Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=60, min_count=10, window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EÄŸitim ve test verilerinizi RNN'e besleyebileceÄŸiniz bir ÅŸeye dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in Ã¶nceki egzersizin fonksiyonlarÄ±nÄ± yeniden kullanalÄ±m.\n",
    "\n",
    "â“ **Soru** â“ AÅŸaÄŸÄ±daki fonksiyonu okuyun, ne olup bittiÄŸinden emin olun ve Ã§alÄ±ÅŸtÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:05:47.784771Z",
     "iopub.status.busy": "2026-02-15T10:05:47.782882Z",
     "iopub.status.idle": "2026-02-15T10:05:53.741709Z",
     "shell.execute_reply": "2026-02-15T10:05:53.738026Z",
     "shell.execute_reply.started": "2026-02-15T10:05:47.784684Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "\n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "\n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â˜ï¸ Ã‡alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olmak iÃ§in, `X_train_pad` ve `X_test_pad` iÃ§in aÅŸaÄŸÄ±dakileri kontrol edelim:\n",
    "- numpy dizileri olmalÄ±lar\n",
    "- 3-boyutlu olmalÄ±lar\n",
    "- son boyut word2vec gÃ¶mÃ¼leme uzayÄ±nÄ±zÄ±n boyutu kadar olmalÄ± (`word2vec.wv.vector_size` ile alabilirsiniz)\n",
    "- ilk boyut `X_train` ve `X_test`'inizin boyutu kadar olmalÄ±\n",
    "\n",
    "âœ… **Ä°yi Uygulama** âœ… Bu tÃ¼r testler oldukÃ§a Ã¶nemli! Sadece bu egzersizde deÄŸil, gerÃ§ek hayat uygulamalarÄ±nda da. HatalarÄ± Ã§ok geÃ§ keÅŸfetmeyi ve tÃ¼m notebook boyunca yayÄ±lmalarÄ±nÄ± Ã¶nler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:05:53.747682Z",
     "iopub.status.busy": "2026-02-15T10:05:53.745658Z",
     "iopub.status.idle": "2026-02-15T10:05:53.760633Z",
     "shell.execute_reply": "2026-02-15T10:05:53.757995Z",
     "shell.execute_reply.started": "2026-02-15T10:05:53.747593Z"
    }
   },
   "outputs": [],
   "source": [
    "# TEST ME\n",
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "Kendi modelinizi test edebileceÄŸiniz Ã§ok basit bir modele sahip olmak her zaman iyidir - Ã§ok basit bir algoritmadan daha iyi bir ÅŸey yaptÄ±ÄŸÄ±nÄ±zdan emin olmak iÃ§in.\n",
    "\n",
    "â“ **Soru** â“ Baseline doÄŸruluÄŸunuz nedir? Bu durumda, baseline'Ä±nÄ±z `y_train`'de en Ã§ok bulunan etiketi tahmin etmek olabilir (tabii ki, veri seti dengeliyse, baseline doÄŸruluÄŸu 1/n'dir, burada n sÄ±nÄ±f sayÄ±sÄ±dÄ±r - burada 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:05:53.765073Z",
     "iopub.status.busy": "2026-02-15T10:05:53.763286Z",
     "iopub.status.idle": "2026-02-15T10:05:54.034799Z",
     "shell.execute_reply": "2026-02-15T10:05:54.032290Z",
     "shell.execute_reply.started": "2026-02-15T10:05:53.764992Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in train set {0: 1265, 1: 1235}\n",
      "Baseline accuracy:  0.492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "counts = dict(zip(unique, counts))\n",
    "print('Number of labels in train set', counts)\n",
    "\n",
    "y_pred = 0 if counts[0] > counts[1] else 1\n",
    "\n",
    "print('Baseline accuracy: ', accuracy_score(y_test, [y_pred]*len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "â“ **Soru** â“ AÅŸaÄŸÄ±daki katmanlarÄ± iÃ§eren bir RNN yazÄ±n:\n",
    "- bir `Masking` katmanÄ±\n",
    "- 20 birim ve `tanh` aktivasyon fonksiyonu ile bir `LSTM`\n",
    "- 10 birim ile bir `Dense`\n",
    "- gÃ¶revinize baÄŸlÄ± bir Ã§Ä±ktÄ± katmanÄ±\n",
    "\n",
    "Sonra, modelinizi derleyin (optimizer olarak `rmsprop` kullanmanÄ±zÄ± tavsiye ederiz - en azÄ±ndan baÅŸlangÄ±Ã§ iÃ§in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:05:54.039872Z",
     "iopub.status.busy": "2026-02-15T10:05:54.038074Z",
     "iopub.status.idle": "2026-02-15T10:05:54.113298Z",
     "shell.execute_reply": "2026-02-15T10:05:54.110122Z",
     "shell.execute_reply.started": "2026-02-15T10:05:54.039743Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(20, activation='tanh'))\n",
    "    model.add(layers.Dense(15, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Soru** â“ Modeli gÃ¶mÃ¼lmÃ¼ÅŸ ve dolgulanmÄ±ÅŸ verileriniz Ã¼zerinde eÄŸitin - early stopping kriterini unutmayÄ±n.\n",
    "\n",
    "â— **Not** â— DoÄŸruluÄŸunuz bÃ¼yÃ¼k Ã¶lÃ§Ã¼de eÄŸitim corpus'unuza baÄŸlÄ± olacak. Burada sadece performansÄ±nÄ±zÄ±n baseline modelin Ã¼zerinde olduÄŸundan emin olun (bu, baÅŸlangÄ±Ã§taki IMDB verilerinin sadece %20'sini yÃ¼klemiÅŸ olsanÄ±z bile bÃ¶yle olmalÄ±)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:05:54.118232Z",
     "iopub.status.busy": "2026-02-15T10:05:54.116753Z",
     "iopub.status.idle": "2026-02-15T10:08:56.798414Z",
     "shell.execute_reply": "2026-02-15T10:08:56.795447Z",
     "shell.execute_reply.started": "2026-02-15T10:05:54.118135Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 207ms/step - accuracy: 0.5189 - loss: 0.6941 - val_accuracy: 0.5387 - val_loss: 0.6890\n",
      "Epoch 2/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 213ms/step - accuracy: 0.5697 - loss: 0.6819 - val_accuracy: 0.5813 - val_loss: 0.6802\n",
      "Epoch 3/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 206ms/step - accuracy: 0.5971 - loss: 0.6705 - val_accuracy: 0.5720 - val_loss: 0.6804\n",
      "Epoch 4/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 157ms/step - accuracy: 0.6074 - loss: 0.6582 - val_accuracy: 0.5800 - val_loss: 0.6813\n",
      "Epoch 5/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 204ms/step - accuracy: 0.6429 - loss: 0.6443 - val_accuracy: 0.6093 - val_loss: 0.6605\n",
      "Epoch 6/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 198ms/step - accuracy: 0.6446 - loss: 0.6334 - val_accuracy: 0.6213 - val_loss: 0.6554\n",
      "Epoch 7/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 173ms/step - accuracy: 0.6463 - loss: 0.6229 - val_accuracy: 0.5853 - val_loss: 0.6585\n",
      "Epoch 8/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 214ms/step - accuracy: 0.6754 - loss: 0.6097 - val_accuracy: 0.5827 - val_loss: 0.6869\n",
      "Epoch 9/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 204ms/step - accuracy: 0.6800 - loss: 0.6018 - val_accuracy: 0.6173 - val_loss: 0.6477\n",
      "Epoch 10/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.6823 - loss: 0.5939 - val_accuracy: 0.6427 - val_loss: 0.6410\n",
      "Epoch 11/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 174ms/step - accuracy: 0.6914 - loss: 0.5825 - val_accuracy: 0.6533 - val_loss: 0.6344\n",
      "Epoch 12/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 170ms/step - accuracy: 0.7040 - loss: 0.5761 - val_accuracy: 0.6613 - val_loss: 0.6278\n",
      "Epoch 13/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 182ms/step - accuracy: 0.7029 - loss: 0.5770 - val_accuracy: 0.6000 - val_loss: 0.7206\n",
      "Epoch 14/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.7194 - loss: 0.5643 - val_accuracy: 0.6520 - val_loss: 0.6345\n",
      "Epoch 15/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 167ms/step - accuracy: 0.7160 - loss: 0.5526 - val_accuracy: 0.6253 - val_loss: 0.6542\n",
      "Epoch 16/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 202ms/step - accuracy: 0.7240 - loss: 0.5523 - val_accuracy: 0.6533 - val_loss: 0.6284\n",
      "Epoch 17/100\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 152ms/step - accuracy: 0.7257 - loss: 0.5538 - val_accuracy: 0.6627 - val_loss: 0.6309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f59230d3e30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train_pad_short = X_train_pad[:500] # These two lines are just to accelerate the cell run\n",
    "#y_train_short = y_train[:500]\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train,\n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Soru** â“ Modelinizi test seti Ã¼zerinde deÄŸerlendirin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:08:56.811420Z",
     "iopub.status.busy": "2026-02-15T10:08:56.809410Z",
     "iopub.status.idle": "2026-02-15T10:09:02.869166Z",
     "shell.execute_reply": "2026-02-15T10:09:02.865809Z",
     "shell.execute_reply.started": "2026-02-15T10:08:56.811362Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 65.920%\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EÄŸitilmiÅŸ Word2Vec - Transfer Learning\n",
    "\n",
    "DoÄŸruluÄŸunuz baseline modelin Ã¼zerinde olmasÄ±na raÄŸmen oldukÃ§a dÃ¼ÅŸÃ¼k olabilir. Veri temizleme ve gÃ¶mÃ¼lemenin kalitesini artÄ±rma gibi bunu iyileÅŸtirmek iÃ§in birÃ§ok seÃ§enek var.\n",
    "\n",
    "Burada veri temizleme stratejilerine girmeyeceÄŸiz. GÃ¶mÃ¼lememizin kalitesini artÄ±rmaya Ã§alÄ±ÅŸalÄ±m. Ama sadece daha bÃ¼yÃ¼k bir corpus yÃ¼klemek yerine, baÅŸkalarÄ±nÄ±n Ã¶ÄŸrendiÄŸi gÃ¶mÃ¼lemeden neden faydalanmayalÄ±m? Ã‡Ã¼nkÃ¼ bir gÃ¶mÃ¼lemenin kalitesi, yani kelimelerin yakÄ±nlÄ±ÄŸÄ±, farklÄ± gÃ¶revlerden tÃ¼retilebilir. Transfer learning tam olarak budur.\n",
    "\n",
    "\n",
    "\n",
    "â“ **Soru** â“ Word2vec'te mevcut olan tÃ¼m farklÄ± modelleri listeleyin: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:09:02.874416Z",
     "iopub.status.busy": "2026-02-15T10:09:02.872736Z",
     "iopub.status.idle": "2026-02-15T10:09:03.237387Z",
     "shell.execute_reply": "2026-02-15T10:09:03.233976Z",
     "shell.execute_reply.started": "2026-02-15T10:09:02.873673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â„¹ï¸ Modellerin listesini ve boyutlarÄ±nÄ± [`gensim-data` repository](https://github.com/RaRe-Technologies/gensim-data#models)'de de bulabilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Soru** â“ Ã–nceden eÄŸitilmiÅŸ word2vec gÃ¶mÃ¼leme uzaylarÄ±ndan birini yÃ¼kleyin.\n",
    "\n",
    "Bunu `api.load(seÃ§tiÄŸiniz-model)` ile yapabilir ve `word2vec_transfer`'da saklayabilirsiniz\n",
    "\n",
    "<details>\n",
    "    <summary>ğŸ’¡ Ä°pucu</summary>\n",
    "    \n",
    "`glove-wiki-gigaword-50` modeli daha kÃ¼Ã§Ã¼k olduÄŸu iÃ§in (65 MB) baÅŸlamak iÃ§in iyi bir aday.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:09:03.241652Z",
     "iopub.status.busy": "2026-02-15T10:09:03.240524Z",
     "iopub.status.idle": "2026-02-15T10:10:20.305628Z",
     "shell.execute_reply": "2026-02-15T10:10:20.303137Z",
     "shell.execute_reply.started": "2026-02-15T10:09:03.241567Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Soru** â“ Vocabulary'nin boyutunu kontrol edin, ayrÄ±ca gÃ¶mÃ¼leme uzayÄ±nÄ±n boyutunu da kontrol edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:10:20.309541Z",
     "iopub.status.busy": "2026-02-15T10:10:20.308335Z",
     "iopub.status.idle": "2026-02-15T10:10:20.323690Z",
     "shell.execute_reply": "2026-02-15T10:10:20.321252Z",
     "shell.execute_reply.started": "2026-02-15T10:10:20.309466Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(word2vec_transfer.key_to_index))\n",
    "print(len(word2vec_transfer['dog']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ `X_train` ve `X_test`'i gÃ¶mleyelim, fonksiyonlarÄ± saÄŸladÄ±ÄŸÄ±mÄ±z ilk soruda olduÄŸu gibi! (`embed_sentence_with_TF` fonksiyonunda derinlemesine girmeyeceÄŸimiz kÃ¼Ã§Ã¼k bir fark var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:10:20.328434Z",
     "iopub.status.busy": "2026-02-15T10:10:20.326666Z",
     "iopub.status.idle": "2026-02-15T10:10:26.441637Z",
     "shell.execute_reply": "2026-02-15T10:10:26.438974Z",
     "shell.execute_reply.started": "2026-02-15T10:10:20.328330Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "\n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "\n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Soru** â“ SonuÃ§larÄ±nÄ±zÄ± dolgulamayÄ± unutmayÄ±n ve bunlarÄ± `X_train_pad_2` ve `X_test_pad_2`'de saklayÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:10:26.446382Z",
     "iopub.status.busy": "2026-02-15T10:10:26.444657Z",
     "iopub.status.idle": "2026-02-15T10:10:28.565615Z",
     "shell.execute_reply": "2026-02-15T10:10:28.562902Z",
     "shell.execute_reply.started": "2026-02-15T10:10:26.446240Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Soru** â“ Bir modeli yeniden baÅŸlatÄ±n ve yeni gÃ¶mÃ¼lmÃ¼ÅŸ (ve dolgulanmÄ±ÅŸ) verileriniz Ã¼zerinde eÄŸitin! Test setinizde deÄŸerlendirin ve Ã¶nceki doÄŸruluÄŸunuzla karÅŸÄ±laÅŸtÄ±rÄ±n.\n",
    "\n",
    "â— **Not** â— Buradaki eÄŸitim biraz zaman alabilir. Sadece 10 epoch hesaplayabilirsiniz (bu **iyi** bir uygulama deÄŸil, sadece Ã§ok uzun beklememek iÃ§in) ve eÄŸitim devam ederken bir sonraki egzersize geÃ§ebilir - ya da mola verebilirsiniz, muhtemelen hak ediyorsunuz ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:10:28.570799Z",
     "iopub.status.busy": "2026-02-15T10:10:28.569080Z",
     "iopub.status.idle": "2026-02-15T10:12:30.248497Z",
     "shell.execute_reply": "2026-02-15T10:12:30.245460Z",
     "shell.execute_reply.started": "2026-02-15T10:10:28.570718Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 182ms/step - accuracy: 0.5171 - loss: 0.6967 - val_accuracy: 0.5547 - val_loss: 0.6840\n",
      "Epoch 2/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 175ms/step - accuracy: 0.5954 - loss: 0.6764 - val_accuracy: 0.6027 - val_loss: 0.6660\n",
      "Epoch 3/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 207ms/step - accuracy: 0.6200 - loss: 0.6528 - val_accuracy: 0.6160 - val_loss: 0.6483\n",
      "Epoch 4/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 179ms/step - accuracy: 0.6457 - loss: 0.6273 - val_accuracy: 0.6760 - val_loss: 0.6150\n",
      "Epoch 5/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 190ms/step - accuracy: 0.6754 - loss: 0.6000 - val_accuracy: 0.6893 - val_loss: 0.5830\n",
      "Epoch 6/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 164ms/step - accuracy: 0.7057 - loss: 0.5759 - val_accuracy: 0.6533 - val_loss: 0.6386\n",
      "Epoch 7/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 175ms/step - accuracy: 0.7086 - loss: 0.5658 - val_accuracy: 0.7480 - val_loss: 0.5425\n",
      "Epoch 8/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 211ms/step - accuracy: 0.7343 - loss: 0.5547 - val_accuracy: 0.7200 - val_loss: 0.5558\n",
      "Epoch 9/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 240ms/step - accuracy: 0.7274 - loss: 0.5424 - val_accuracy: 0.7387 - val_loss: 0.5450\n",
      "Epoch 10/10\n",
      "\u001b[1m55/55\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 182ms/step - accuracy: 0.7383 - loss: 0.5397 - val_accuracy: 0.7387 - val_loss: 0.5445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f58e9fcf590>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model = init_model()\n",
    "\n",
    "model.fit(X_train_pad_2, y_train,\n",
    "          batch_size = 32,\n",
    "          epochs=10,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T10:12:30.252626Z",
     "iopub.status.busy": "2026-02-15T10:12:30.251731Z",
     "iopub.status.idle": "2026-02-15T10:12:34.845509Z",
     "shell.execute_reply": "2026-02-15T10:12:34.842397Z",
     "shell.execute_reply.started": "2026-02-15T10:12:30.252523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 72.640%\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test_pad_2, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeni word2vec'iniz bÃ¼yÃ¼k bir corpus Ã¼zerinde eÄŸitildiÄŸi iÃ§in, Ã§ok Ã§ok fazla kelime iÃ§in temsile sahip! KÃ¼Ã§Ã¼k veri setinizle olduÄŸundan Ã§ok daha fazla, Ã¶zellikle eÄŸitim setinde belirli bir sayÄ±dan fazla bulunmayan kelimeleri attÄ±ÄŸÄ±nÄ±z iÃ§in. Bu nedenle, eÄŸitim ve test setinizde Ã§ok daha fazla gÃ¶mÃ¼lmÃ¼ÅŸ kelime var, bu da her iterasyonu Ã¶ncekinden daha uzun hale getiriyor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
